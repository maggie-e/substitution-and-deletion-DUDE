{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from pybrain.optimization import CMAES\n",
    "\n",
    "global kmers\n",
    "global orig\n",
    "global noisy\n",
    "global k\n",
    "global n\n",
    "global alphabet\n",
    "global delta\n",
    "global rho\n",
    "global max_del\n",
    "\n",
    "def allklength(k, alphabet):\n",
    "    allklengthHelper(k, alphabet, \"\")\n",
    "\n",
    "def allklengthHelper(k, alphabet, prefix):\n",
    "    if k == 0:\n",
    "        global kmers\n",
    "        kmers.append(prefix)\n",
    "        return\n",
    "    for i in range(len(alphabet)):\n",
    "        new_prefix = prefix + alphabet[i]\n",
    "        allklengthHelper(k-1, alphabet, new_prefix)\n",
    "\n",
    "def takeReads(genome):\n",
    "    concatenated = ''\n",
    "    for i in range(num_reads):\n",
    "        index = int(random.random()*(len(genome)-read_length))\n",
    "        concatenated += genome[index:index+read_length]\n",
    "    return concatenated\n",
    "\n",
    "def calculateMatrix(input_sequence, k, alphabet):\n",
    "    matrix_dim = len(alphabet)**k;\n",
    "    transition_matrix = np.zeros((matrix_dim, matrix_dim))\n",
    "    allklength(k, alphabet)\n",
    "    sequence_map = {}\n",
    "    for i in range(len(kmers)):\n",
    "        sequence_map[kmers[i]] = i\n",
    "    print(sequence_map)\n",
    "    for i in range(len(input_sequence)-k-2):\n",
    "        current = input_sequence[i:i+k]\n",
    "        next = input_sequence[i+1:i+k+1]\n",
    "        transition_matrix[sequence_map[current], sequence_map[next]] += 1\n",
    "    for i in range(matrix_dim):\n",
    "        norm_factor = sum(transition_matrix[i, 0:matrix_dim-1])\n",
    "        for j in range(matrix_dim):\n",
    "            transition_matrix[i, j] = transition_matrix[i, j]/(norm_factor+0.0)\n",
    "    return transition_matrix, sequence_map\n",
    "\n",
    "def calculateDistribution(input_sequence, k, alphabet):\n",
    "    context_hist = {}\n",
    "    for i in range(k, len(input_sequence)-k):\n",
    "        context = input_sequence[i-k:i]+input_sequence[i+1:i+k+1]\n",
    "        if context in context_hist:\n",
    "            context_hist[context] += input_sequence[i]\n",
    "        else:\n",
    "            context_hist[context] = [input_sequence[i]]\n",
    "    return context_hist\n",
    "        \n",
    "def erasureChannel(input_sequence, deletion_rate):\n",
    "    noisy = input_sequence\n",
    "    for i in range(len(input_sequence)):\n",
    "        x = random.random()\n",
    "        if x < deletion_rate:\n",
    "            noisy = noisy[:i]+'E'+noisy[i+1:]\n",
    "    return noisy\n",
    "\n",
    "def deletionChannel(input_sequence, deletion_rate):\n",
    "    indices = []\n",
    "    for i in range(len(input_sequence)):\n",
    "        x = random.random()\n",
    "        if x < deletion_rate:\n",
    "            indices += [i]\n",
    "    noisy = \"\".join([char for index, char in enumerate(input_sequence) if index not in indices])\n",
    "    return noisy\n",
    "\n",
    "def erasureDenoise(input_sequence, k, alphabet, deletion_rate):\n",
    "    noisy = erasureChannel(input_sequence, deletion_rate)\n",
    "    contexts = calculateDistribution(noisy, k, alphabet)\n",
    "    ml = alphabet[0]\n",
    "    pct = 0\n",
    "    for l in alphabet:\n",
    "        if sum([int(x == l) for x in noisy])/len(noisy) > pct:\n",
    "            pct = sum(noisy == l)/len(noisy)\n",
    "            ml = l\n",
    "    most_common = ml\n",
    "    erasure_corrections = []\n",
    "    for j in range(k):\n",
    "        if noisy[j] == 'E':\n",
    "            erasure_corrections += most_common\n",
    "    for i in range(k, len(noisy)-k):\n",
    "        if noisy[i] == 'E':\n",
    "            p = 0\n",
    "            ml = most_common\n",
    "            context = noisy[i-k:i]+noisy[i+1:i+k+1]\n",
    "            context_hist = contexts[context]\n",
    "            for a in alphabet:\n",
    "                new_p = sum([int(x == a) for x in context_hist])/len(context_hist)\n",
    "                if new_p > p:\n",
    "                    p = new_p\n",
    "                    ml = a\n",
    "            erasure_corrections += ml\n",
    "    for m in range(len(noisy)-k, len(noisy)):\n",
    "        if noisy[m] == 'E':\n",
    "            erasure_corrections += most_common\n",
    "    j = 0\n",
    "    for i in range(len(noisy)):\n",
    "        if noisy[i] == 'E':\n",
    "            noisy = noisy[:i]+erasure_corrections[j]+noisy[i+1:]\n",
    "            j += 1\n",
    "    return noisy\n",
    "\n",
    "def levenshtein(source, target):\n",
    "    if len(source) < len(target):\n",
    "        return levenshtein(target, source)\n",
    "    if len(target) == 0:\n",
    "        return len(source)\n",
    "    source = np.array(tuple(source))\n",
    "    target = np.array(tuple(target))\n",
    "    previous_row = np.arange(target.size + 1)\n",
    "    for s in source:\n",
    "        current_row = previous_row + 1\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                np.add(previous_row[:-1], target != s))\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                current_row[0:-1] + 1)\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n",
    "def error(source, target):\n",
    "    if len(source) < len(target):\n",
    "        return error(target, source)\n",
    "    if len(target) == 0:\n",
    "        return len(source)\n",
    "    errors = 0\n",
    "    j = 0\n",
    "    for i in range(len(source)):\n",
    "        if j >= len(target):\n",
    "            errors += 1\n",
    "        elif source[i] != target[j]:\n",
    "            errors += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return errors\n",
    "\n",
    "def denoiseSequence1(input_sequence, k, alphabet, deletion_rate):\n",
    "    noisy = deletionChannel(input_sequence, deletion_rate)\n",
    "    pi, seq_map = calculateMatrix(noisy, k, alphabet)\n",
    "    denoised = noisy[:k]\n",
    "    for i in range(k-1, len(noisy)-1-k):\n",
    "        base_prob = 1-deletion_rate\n",
    "        next_char = noisy[i+1]\n",
    "        subseq = noisy[i+1-k:i+k+1]\n",
    "        for j in range(k):\n",
    "            base_prob *= pi[seq_map[subseq[j:j+k]], seq_map[subseq[j+1:j+k+1]]]\n",
    "        max_prob = base_prob\n",
    "        for a in alphabet:\n",
    "            insert_prob = deletion_rate\n",
    "            new_subseq = subseq[:k+1]+a+subseq[k+1:]\n",
    "            for m in range(k+1):\n",
    "                insert_prob *= pi[seq_map[new_subseq[m:m+k]], seq_map[new_subseq[m+1:m+k+1]]]\n",
    "            if insert_prob > max_prob:\n",
    "                max_prob = insert_prob\n",
    "                next_char = a + noisy[i+1]\n",
    "        denoised += next_char\n",
    "    for i in range(len(noisy)-1-k, len(noisy)):\n",
    "        denoised += noisy[i]\n",
    "    return input_sequence, noisy, denoised\n",
    "\n",
    "def denoiseSequence2(noisy, k, alphabet, deletion_rate, max_del=1, weights=[1]):\n",
    "    adjust = 1\n",
    "    if len(weights) != max_del:\n",
    "        #print('Error: dimension of weights vector is not equal to number of separation lengths.')\n",
    "        #print('Using default weights.')\n",
    "        weights = [1]*maxdel\n",
    "    context_hists = []\n",
    "    for j in range(max_del):\n",
    "        context_del_hist = {}\n",
    "        for i in range(k+j, len(noisy)-k):\n",
    "            context_del = noisy[i-k-j:i-j]+noisy[i:i+k]\n",
    "            deleted = noisy[i-j:i]\n",
    "            if context_del in context_del_hist:\n",
    "                context_del_hist[context_del].append(deleted)\n",
    "            else:\n",
    "                context_del_hist[context_del] = [deleted]\n",
    "        context_hists.append(context_del_hist)\n",
    "    for j in range(1, max_del):\n",
    "        allklength(j, alphabet)\n",
    "        for i in range(k, len(noisy)-k):\n",
    "            context = noisy[i-k:i+k]\n",
    "            context_del_hist = context_hists[j]\n",
    "            context_hist = context_hists[0]\n",
    "            if context in context_hist and context in context_del_hist:\n",
    "                if adjust*(deletion_rate**j)*len(context_hist[context])*weights[j-1] >= (1.0/adjust)*(1-deletion_rate)*len(context_del_hist[context]):\n",
    "                    ml = kmers[0]\n",
    "                    p = 0\n",
    "                    for a in kmers:\n",
    "                        new_p = sum([int(x == a) for x in context_hist[context]])/(len(context_hist[context])+0.0)\n",
    "                        if new_p > p:\n",
    "                            p = new_p\n",
    "                            ml = a\n",
    "                    noisy = noisy[:i]+ml+noisy[i:]\n",
    "        global kmers\n",
    "        kmers = []\n",
    "    return noisy\n",
    "    \n",
    "def optimalDenoise(noisy, k, alphabet, rho, alpha):\n",
    "    for i in range(len(noisy)-1):\n",
    "        if noisy[i] == noisy[i+1] and rho*alpha**2/(1-alpha) > 1:\n",
    "            if noisy[i] == alphabet[0]:\n",
    "                noisy = noisy[:i+1]+alphabet[1]+noisy[i+1:]\n",
    "            else:\n",
    "                noisy = noisy[:i+1]+alphabet[0]+noisy[i+1:]\n",
    "    return noisy\n",
    "        \n",
    "\n",
    "def denoiseSequence3(noisy, k, alphabet, rho, l=-1, weights=[1]):\n",
    "    adjust = 1\n",
    "    if l == -1:\n",
    "        l = k\n",
    "    if len(weights) != l:\n",
    "        #print('Error: dimension of weights vector is not equal to number of votes per context.')\n",
    "        #print('Using default weights.')\n",
    "        weights = [1]*l\n",
    "    context_left = {} \n",
    "    context_right = {}\n",
    "    for i in range(len(noisy)):\n",
    "        if i >= k:\n",
    "            lcontext = noisy[i-k:i]\n",
    "            if lcontext in context_left:\n",
    "                context_left[lcontext] += noisy[i]\n",
    "            else:\n",
    "                context_left[lcontext] = [noisy[i]]\n",
    "        if i < len(noisy)-k:\n",
    "            rcontext = noisy[i+1:i+k+1]\n",
    "            if rcontext in context_right:\n",
    "                context_right[rcontext] += noisy[i]\n",
    "            else:\n",
    "                context_right[rcontext] = [noisy[i]]\n",
    "    for i in range(k, len(noisy)-k-1):\n",
    "        context1 = noisy[i-k+1:i+k+2]\n",
    "        for a in alphabet:\n",
    "            vote = 0\n",
    "            context2 = noisy[i-k+1:i+1]+a+noisy[i+1:i+k+1]\n",
    "            if a+rcontext[:-1] in context_right and lcontext[1:]+a in context_left:\n",
    "                for x in range(l):\n",
    "                    if context2[i-x+1:i-x+k] in context_right:\n",
    "                        p1 = sum([int(y == noisy[i-x]) for y in context_right[context1[i-x+1:i-x+k+1]]])\n",
    "                        p2 = sum([int(y == noisy[i-x]) for y in context_right[context2[i-x+1:i-x+k+1]]])\n",
    "                        if (1-rho)*p1 <= rho*p2:\n",
    "                            vote += weights[x]\n",
    "                for x in range(l):\n",
    "                    if context2[i+x-k:i+x] in context_left:\n",
    "                        p1 = sum([int(y == noisy[i+x]) for y in context_left[context1[i+x-k:i+x]]]) \n",
    "                        p2 = sum([int(y == noisy[i+x]) for y in context_left[context2[i+x-k:i+x]]])\n",
    "                        if (1-rho)*p1 <= rho*p2:\n",
    "                            vote += weights[x]\n",
    "                if vote >= l:\n",
    "                    noisy = noisy[:i+1]+a+noisy[i+1:]\n",
    "    return noisy\n",
    "\n",
    "def textDenoise(filename):\n",
    "    n = 10000\n",
    "    ks = open(filename, 'r').read()[:n]\n",
    "    k = int(0.5*math.log(n, 3))\n",
    "    eps = 0.1\n",
    "    alphabet = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
    "    noisy = deletionChannel(ks, eps)\n",
    "    est1 = denoiseSequence2(noisy, k, alphabet, eps)\n",
    "    f = open(filename+'_denoised_1', 'w')\n",
    "    f.write(est1)\n",
    "    f.close()\n",
    "    f = open(filename+'_noisy', 'w')\n",
    "    f.write(noisy)\n",
    "    f.close()\n",
    "\n",
    "def weightWrapper1(weights):\n",
    "    est1 = denoiseSequence2(noisy, k, alphabet, rho, max_del, weights)\n",
    "    est = optimalDenoise(noisy, k, alphabet, rho, delta)\n",
    "    err = error(est1, orig)/(n+0.0)\n",
    "    return err\n",
    "    \n",
    "def weightWrapper2(weights):\n",
    "    est2 = denoiseSequence3(noisy, k, alphabet, rho, max_del, weights)\n",
    "    est = optimalDenoise(noisy, k, alphabet, rho, delta)\n",
    "    err = error(est2, orig)/(n+0.0)\n",
    "    return err\n",
    "\n",
    "def generateSequence(n, a):\n",
    "    p = random.random()\n",
    "    global k\n",
    "    k = int(0.5*math.log(n, 3))\n",
    "    x = ''\n",
    "    for i in range(n):\n",
    "        if i == 0:\n",
    "            if p < 0.5:\n",
    "                x += '+'\n",
    "            else:\n",
    "                x += '-'\n",
    "        else:\n",
    "            if p < a:\n",
    "                if x[i-1] == '+':\n",
    "                    x += '-'\n",
    "                else:\n",
    "                    x += '+'\n",
    "            else:\n",
    "                x += x[i-1]\n",
    "            p = random.random()\n",
    "    return x\n",
    "\n",
    "def markovSourceDenoise(transition_prob, deletion_rate, weighted=False):\n",
    "    global delta\n",
    "    delta = transition_prob\n",
    "    global rho\n",
    "    rho = deletion_rate\n",
    "    global max_del\n",
    "    max_del = 1\n",
    "    global n\n",
    "    n = 10000\n",
    "    display = 50\n",
    "    global alphabet\n",
    "    alphabet = ['+', '-']\n",
    "    global orig\n",
    "    orig = generateSequence(n, delta)\n",
    "    global noisy\n",
    "    noisy = deletionChannel(orig, rho)\n",
    "    est1 = denoiseSequence2(noisy, k, alphabet, rho)\n",
    "    est2 = denoiseSequence3(noisy, k, alphabet, rho)\n",
    "    est = optimalDenoise(noisy, k, alphabet, rho, delta)\n",
    "    print 'Setting: delta = ', delta, ', rho = ', rho \n",
    "    print 'Original: ', orig[:display], '(length ', len(orig), ' error ', error(orig, orig)/(n+0.0), ')'\n",
    "    print 'Noisy: ', noisy[:display], '(length ', len(noisy), ' error ', error(noisy, orig)/(n+0.0), ')'\n",
    "    print 'Denoised: ', est[:display], '(length ', len(est), ' error ', error(est, orig)/(n+0.0), ' )'\n",
    "    print 'Denoiser 1: ', est1[:display], '(length ', len(est1), ' error ', error(est1, orig)/(n+0.0), ')'\n",
    "    print 'Denoiser 2: ', est2[:display], '(length ', len(est2), ' error ', error(est2, orig)/(n+0.0), ')'\n",
    "    if weighted:\n",
    "        l1 = CMAES(weightWrapper1, [1])\n",
    "        l1.minimize = True\n",
    "        l1.maxEvaluations = 200\n",
    "        weights1 = l1.learn()[0]\n",
    "        l2 = CMAES(weightWrapper2, [1]*k)\n",
    "        l2.minimize = True\n",
    "        l2.maxEvaluations = 200\n",
    "        weights2 = l2.learn()[0]\n",
    "        weighted_est1 = denoiseSequence2(noisy, k, alphabet, rho, max_del, weights1)\n",
    "        weighted_est2 = denoiseSequence3(noisy, k, alphabet, rho, max_del, weights2)\n",
    "        print 'Weighted Denoiser 1: ', weighted_est1[:display], '(length ', len(weighted_est1), ' error ', error(weighted_est1, orig)/(n+0.0), ')'\n",
    "        print 'Weighted Denoiser 2: ', weighted_est2[:display], '(length ', len(weighted_est2), ' error ', error(weighted_est2, orig)/(n+0.0), ')'\n",
    "    print '\\n'*5\n",
    "\n",
    "markovSourceDenoise(0.9, 0.2, True)\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
